{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vISGbxrDeGdd"
   },
   "source": [
    "<div>\n",
    "    <h1 id=\"Understanding_the_Power_of_the_Catalyst_Optimizer_in_PySpark\">üöÄ Understanding the Power of the Catalyst Optimizer in PySpark üîç</h1>\n",
    "    <p>When working with large-scale data processing in Apache Spark, the Catalyst Optimizer is one of the most powerful features that drives query optimization and enhances the overall performance of your Spark jobs.</p>\n",
    "    <div>\n",
    "        <h2>üåüWhat is the Catalyst Optimizer?</h2>\n",
    "        <div>\n",
    "            <p>The Catalyst Optimizer is a key component of the Spark SQL engine. It is responsible for optimizing your SQL and DataFrame queries through a series of transformations, including:</p>\n",
    "            <div>\n",
    "                <div>\n",
    "                    <ol>\n",
    "                        <li>\n",
    "                            <p><b>Analysis</b>: Checks whether the query is semantically correct.</p>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <p><b>Logical Optimization</b>: Applies rules like constant folding and predicate pushdown.</p>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <p><b>Physical Planning</b>: Decides on the most efficient execution plan for the query.</p>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <p><b>Code Generation</b>:  Converts the optimized plan into physical execution code.</p>\n",
    "                        </li>\n",
    "                    </ol>\n",
    "                </div>\n",
    "            </div>\n",
    "            <p>With the Catalyst Optimizer, Spark can significantly reduce the computational cost of queries and improve query execution times, even with large datasets.</p>\n",
    "        </div>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h2>üîßHow Does Catalyst Make Spark Fast?</h2>\n",
    "        <div>\n",
    "            <p></p>\n",
    "            <div>\n",
    "                <div>\n",
    "                    <ol>\n",
    "                        <li>\n",
    "                            <p><b>Query Rewriting</b>: Automatically rewrites queries for optimization (like reordering joins or eliminating redundant filters).</p>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <p><b>Predicate Pushdown</b>: Filters data as early as possible to avoid unnecessary data shuffling.</p>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <p><b>Cost-based Optimization</b>: Chooses the most efficient query execution plan based on cost estimation.</p>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <p><b>Advanced Rule Application</b>: Leverages a set of rules to optimize queries based on patterns.</p>\n",
    "                        </li>\n",
    "                    </ol>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h2>üí°Why should we care?</h2>\n",
    "        <div>\n",
    "            <p>Catalyst helps Spark scale to massive datasets while optimizing execution time.</p>\n",
    "            <p>It simplifies complex query optimization, making it easier for developers to focus on writing the logic without worrying about performance bottlenecks.</p>\n",
    "        </div>\n",
    "    </div>\n",
    "    <div>\n",
    "        <h2>‚ö° Pro Tip</h2>\n",
    "        <div>\n",
    "            <p>Always keep an eye on the execution plans with <code>.explain()</code> to understand how Spark's Catalyst Optimizer is optimizing your queries! If we are working with PySpark and SQL queries, understanding how the Catalyst Optimizer works can be a game-changer for building efficient and performant data pipelines.</p>\n",
    "            <p></p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UuXlT-ygRXx"
   },
   "source": [
    "<div>\n",
    "    <h1>Catalyst Optimizer in Pyspark</h1>\n",
    "    <div>\n",
    "        <h2></h2>\n",
    "        <div>\n",
    "            <p>The Catalyst Optimizer is a core component of Apache Spark's SQL engine, specifically designed to optimize queries and DataFrame operations in PySpark. It improves the performance of data processing tasks by transforming logical query plans into optimized physical query plans</p>\n",
    "            <img src=\"https://github.com/aashish22bansal/Python-Programming/blob/main/Images/Complete_PySpark/Catalyst%20Optimizer%20in%20PySpark%20-%20Image%201%20-%20Architecture.png?raw=true:, width=100\" alt=\"My Image\">\n",
    "            <!-- <img src=\"https://github.com/aashish22bansal/Python-Programming/blob/main/Images/Complete_PySpark/Catalyst%20Optimizer%20in%20PySpark%20-%20Image%201%20-%20Architecture.png?raw=true:, width=100\" alt=\"My Image\" width=200> -->\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4K3rolpkLgi"
   },
   "source": [
    "<div>\n",
    "    <div>\n",
    "        <h2>What is the Catalyst Optimizer?</h2>\n",
    "        <div>\n",
    "            <p>The Catalyst Optimizer is a query optimization framework used by <b>Spark SQL</b> to optimize SQL queries and DataFrame operations. It is part of the query execution engine in Apache Spark and applies various optimization techniques to enhance the performance of queries, such as predicate pushdown, constant folding, and filter pushdown.</p>\n",
    "            <p>The Catalyst Optimizer works in three stages:</p>\n",
    "            <div>\n",
    "                <div>\n",
    "                    <ol>\n",
    "                        <li>\n",
    "                            <p><b>Analysis</b>: It checks the syntax of the query and resolves it into a logical plan by referring to the schema of the data.</p>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <p><b>Logical Optimization</b>: This stage applies various optimizations (e.g., removing redundant operations, simplifying expressions).</p>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <p><b>Physical Planning</b>: It generates different physical execution plans and chooses the most efficient one based on cost estimation (e.g., partitioning, parallelism, etc.).</p>\n",
    "                        </li>\n",
    "                    </ol>\n",
    "                </div>\n",
    "            </div>\n",
    "            <p>Key Features of Catalyst Optimizer:</p>\n",
    "            <div>\n",
    "                <div>\n",
    "                    <ol>\n",
    "                        <li>\n",
    "                            <p><b>Query Transformation</b>: It can apply a wide range of transformations like <b>predicate pushdown</b>, <b>constant folding</b>, and <b>join reordering</b>.</p>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <p><b>Cost-Based Optimization (CBO)</b>: The optimizer can estimate the cost of different query plans and choose the one that minimizes the computation cost.</p>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <p><b>Rule-Based Optimization</b>: Catalyst uses a set of transformation rules to optimize queries based on patterns (e.g., converting JOIN into HASH JOIN where applicable).</p>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <p><b>Physical Planning</b>: Catalyst generates multiple physical plans and selects the one with the lowest cost, considering things like partitioning, shuffle operations, and other physical factors.</p>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <p><b>Extensibility</b>: Catalyst allows for easy extension, meaning developers can add custom optimization rules if needed, making it a powerful and flexible optimizer.</p>\n",
    "                        </li>\n",
    "                    </ol>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0usMzuqckO-q"
   },
   "source": [
    "<div>\n",
    "    <div>\n",
    "        <h2>Example of Optimizations Applied by Catalyst</h2>\n",
    "        <div>\n",
    "            <p><b>Predicate Pushdown</b>: If you have filters applied on columns, Catalyst will push these filters down to the data source level (like HDFS, Parquet, etc.), reducing the amount of data read. Catalyst can push the filter operation down to the source (like a Parquet file), reading only the necessary data.</p>\n",
    "            <p></p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKrOy5k-mWDE",
    "outputId": "42fe9bb5-8a75-4bfc-ae02-6abd74ba908e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Name|\n",
      "+----+\n",
      "|   C|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.Age > 30).select(\"Name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLaWghgUkPdK"
   },
   "source": [
    "<div>\n",
    "    <div>\n",
    "        <div>\n",
    "            <p><b>Projection Pruning</b>: If you're selecting only a few columns from a DataFrame, Catalyst can optimize the query to read only those columns rather than the entire row.</p>\n",
    "            <p><b>Join Optimization</b>: Catalyst applies optimizations for join types (e.g., transforming a shuffle join into a broadcast join if one of the tables is small enough).</p>\n",
    "            <p><b>Constant Folding</b>: It evaluates expressions with constant values at compile time, reducing runtime calculations. Catalyst would optimize this to the constant value 8 before executing the query.</p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "Trx5QfKim336",
    "outputId": "53e5dfb0-0e34-40f6-9c1b-e2112d01d040"
   },
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-898a7fca042f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3227\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3228\u001b[0m         \"\"\"\n\u001b[0;32m-> 3229\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_jcols\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2766\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2768\u001b[0m     def _sort_cols(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_jseq\u001b[0;34m(self, cols, converter)\u001b[0m\n\u001b[1;32m   2751\u001b[0m     ) -> JavaObject:\n\u001b[1;32m   2752\u001b[0m         \u001b[0;34m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2753\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJavaObject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         raise PySparkTypeError(\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0merror_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"NOT_COLUMN_OR_STR\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int."
     ]
    }
   ],
   "source": [
    "df.select(5 + 3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNIzNeYxkPx6"
   },
   "source": [
    "<div>\n",
    "    <h1>Example in PySpark</h1>\n",
    "    <div>\n",
    "        <div>\n",
    "            <p>Here's an example of how Catalyst Optimizer works with PySpark:</p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CM5leHXRnG4b"
   },
   "outputs": [],
   "source": [
    "# Import Library\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOgpgIrenLQk"
   },
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"Catalyst Optimizer Example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8sT8WZ-nUDE"
   },
   "outputs": [],
   "source": [
    "# Sample Data\n",
    "data = [\n",
    "    (\"A\", 25),\n",
    "    (\"B\", 30),\n",
    "    (\"C\", 35)\n",
    "]\n",
    "\n",
    "columns = [\"Name\", \"Age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-1B7cojniK0"
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYYc08NZnrji"
   },
   "outputs": [],
   "source": [
    "# Optimized DataFrame Transformations\n",
    "optimized_df = df.filter(df.Age > 25).select(\"Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezBO54Awnzt0",
    "outputId": "f600e857-80f5-468e-9d4c-897c4fd43836"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Name|\n",
      "+----+\n",
      "|   B|\n",
      "|   C|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show Results\n",
    "optimized_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tpin2693m84o"
   },
   "source": [
    "<div>\n",
    "    <div>\n",
    "        <div>\n",
    "            <p>In the background, Catalyst will optimize the filter operation and only read the data that satisfies <code>Age > 25</code>. If possible, it might even push down the Age filter to reduce the amount of data being loaded into memory</p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zywu4-4-m890"
   },
   "source": [
    "<div>\n",
    "    <div>\n",
    "        <h2>Benefits of Using the Catalyst Optimizer</h2>\n",
    "        <div>\n",
    "            <ol>\n",
    "                <li>\n",
    "                    <p><b>Performance Improvements</b>: By applying various optimizations, it ensures that queries execute faster.</p>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <p><b>Automatic Optimization</b>: You don‚Äôt need to manually optimize your queries; Catalyst applies many standard optimizations automatically</p>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <p><b>Scalability</b>: Catalyst makes it possible to handle large-scale data more efficiently by minimizing unnecessary operations like shuffling and scans.</p>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <p><b>Extensibility</b>: : If you have specific optimization needs, you can extend Catalyst with custom optimization rules.</p>\n",
    "                </li>\n",
    "            </ol>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3Ytwpeam9Ap"
   },
   "source": [
    "<div>\n",
    "    <div>\n",
    "        <h2>Conclusion</h2>\n",
    "        <div>\n",
    "            <p>The <b>Catalyst Optimizer</b> is a key part of PySpark's SQL execution engine, automating many optimizations that would otherwise require manual tuning. By applying optimizations like predicate pushdown, constant folding, and join optimizations, Catalyst improves the performance of DataFrame and SQL queries in Spark, making it essential for building efficient and scalable data processing applications.</p>\n",
    "            <p></p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7Vvql7Km9DH"
   },
   "source": [
    "<div>\n",
    "    <div>\n",
    "        <h2></h2>\n",
    "        <div>\n",
    "            <p></p>\n",
    "            <p></p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iP_bSHKLm9FV"
   },
   "source": [
    "<div>\n",
    "    <div>\n",
    "        <h2></h2>\n",
    "        <div>\n",
    "            <p></p>\n",
    "            <p></p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEzbRQ_Gm9Hm"
   },
   "source": [
    "<div>\n",
    "    <div>\n",
    "        <h2></h2>\n",
    "        <div>\n",
    "            <p></p>\n",
    "            <p></p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zMIsWHdm9Kd"
   },
   "source": [
    "<div>\n",
    "    <div>\n",
    "        <h2></h2>\n",
    "        <div>\n",
    "            <p></p>\n",
    "            <p></p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tuh3CsRsm9MZ"
   },
   "source": [
    "<div>\n",
    "    <div>\n",
    "        <h2></h2>\n",
    "        <div>\n",
    "            <p></p>\n",
    "            <p></p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQVjXqzSm9O-"
   },
   "source": [
    "<div>\n",
    "    <div>\n",
    "        <h2></h2>\n",
    "        <div>\n",
    "            <p></p>\n",
    "            <p></p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mm3FdWKlm9RQ"
   },
   "source": [
    "<div>\n",
    "    <div>\n",
    "        <h2></h2>\n",
    "        <div>\n",
    "            <p></p>\n",
    "            <p></p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCrXoxw-kQM6"
   },
   "source": [
    "<div>\n",
    "    <div>\n",
    "        <h2></h2>\n",
    "        <div>\n",
    "            <p></p>\n",
    "            <p></p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
